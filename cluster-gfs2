On Both node 
On both the nodes
dnf -y install pacemaker pcs fence-agents-ilo* lvm2-lockd gfs2-utils dlm

systemctl enable pcsd
systemctl start  pcsd

echo "redhat" | passwd --stdin hacluster
pcs host auth node1 node2
pcs cluster setup nfscluster node1 node2


set the use_lvmlockd configuration option in the /etc/lvm/lvm.conf file to use_lvmlockd=1.

pcs host auth -u hacluster -p redhat node1 node2
pcs cluster setup appgfs --start node1 node2 --force
pcs cluster start --all
pcs cluster enable --all
pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=192.168.122.120 --group shared_vg1 
pcs stonith create node1_ipmi fence_ipmilan ipaddr=192.168.11.11 lanplus=1 login=unixadmin passwd='redhat' pcmk_monitor_timeout=120s
pcs stonith create node2_ipmi fence_ipmilan ipaddr=192.168.11.12 lanplus=1 login=unixadmin passwd='redhat' pcmk_monitor_timeout=120s
pcs stonith update node1_ipmi delay=10
pcs cluster config update totem token=120000
pcs property set no-quorum-policy=freeze
pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence
pcs resource clone locking interleave=true
pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence
vgcreate --shared shared_vg1 /dev/mapper/mpatha
vgchange --lock-start shared_vg1
lvcreate --activate sy -l 100%FREE -n shared_lv1 shared_vg1
mkfs.gfs2 -j2 -p lock_dlm -t ha:gfs2 /dev/shared_vg1/shared_lv1
pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd
pcs resource clone shared_vg1 interleave=true
pcs constraint order start locking-clone then shared_vg1-clone
pcs constraint colocation add shared_vg1-clone with locking-clone
pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv1" directory="/shared" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence
lvs
df -HTP
mount |grep gfs
pcs status --full

https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-gfs2-in-a-cluster-configuring-and-managing-high-availability-clusters

pcs property set no-quorum-policy=freeze
pcs resource create dlm --group locking ocf:pacemaker:controld op monitor interval=30s on-fail=fence
pcs resource clone locking interleave=true
pcs resource create lvmlockd --group locking ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence
vgcreate --shared shared_vg1 /dev/vdb
If you are using an LVM devices file, supported in RHEL 8.5 and later, add the shared devices to the devices file.

lvmdevices --adddev /dev/vdb
vgchange --lockstart shared_vg1
lvcreate --activate sy -l 100%FREE -n shared_lv1 shared_vg1
mkfs.gfs2 -j2 -p lock_dlm -t my_cluster:gfs2-demo1 /dev/shared_vg1/shared_lv1
pcs resource create sharedlv1 --group shared_vg1 ocf:heartbeat:LVM-activate lvname=shared_lv1 vgname=shared_vg1 activation_mode=shared vg_access_mode=lvmlockd
pcs resource clone shared_vg1 interleave=true
pcs constraint order start locking-clone then shared_vg1-clone
pcs constraint colocation add shared_vg1-clone with locking-clone
lvs
pcs resource create sharedfs1 --group shared_vg1 ocf:heartbeat:Filesystem device="/dev/shared_vg1/shared_lv1" directory="/mnt/gfs1" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence
mount | grep gfs2


